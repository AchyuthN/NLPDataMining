# Importing necessary modules
import requests
from bs4 import BeautifulSoup
import nltk
#Obtaining connection to URL
url = requests.get('https://www.thehindu.com/todays-paper/')
urltext = url.text
soup = BeautifulSoup(urltext)
# Parsing the text related to headlines of 17/12/2018 from The Hindu
links = soup.find('div', {'class':'col-lg-12 col-md-12 col-sm-12 col-xs-12'}).text
links
word_token = nltk.word_tokenize(links)
word_token
import nltk
nltk.download('stopwords')
sw= nltk.corpus.stopwords.words('english')
swi = []
for iu in word_token:
    if iu not in sw:
        swi.append(iu)
print(swi)
# POS Tagging
nltk.pos_tag(swi[:10])
# TfidfVectorizer for finding the rare and the most common words. This shall be a decision making factor in keyword search
from sklearn.feature_extraction.text import TfidfVectorizer
vectorizer = TfidfVectorizer()
s1 = s1.join(swi[:9])
s2 = s2.join(swi[10:15])
result = vectorizer.fit_transform([s1, s2])
print(result)




